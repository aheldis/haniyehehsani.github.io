---
layout: homepage
---

## About Me
I am a CS Ph.D. student at University of Maryland since Spring 2023, advised by <a href="https://www.cs.umd.edu/~sfeizi/">Prof. Soheil Feizi</a>.
  My research mainly revolves around Foundation and Multimodal Models,
  covering topics such as image generation, vision-language models, and interpretability and robustness of machine learning methods.

## Research Interests

- **Generative AI:** diffusion models, multimodal models, large language models.
- **Trustworthy AI:** adversarial robustness, deepfake detection.
- **AI Interpretability:** spurious features, failure-mode detection.

## News
- **[Aug. 2024]** I've been honored to be part of the organizer team for the <a href="https://openreview.net/forum?id=LYvWVFdGZN&referrer=%5Bthe%20profile%20of%20Furong%20Huang%5D(%2Fprofile%3Fid%3D~Furong_Huang1)">Erasing the Invisible: A Stress-Test Challenge for Image Watermarks</a> competition at Neurips 2024.
- **[May. 2024]** Check out <a href="https://eprint.iacr.org/2024/855">our new work</a> on the risks and opportunities of modern generative AI.
- **[Mar. 2024]** I had the opportunity to present our work "Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks" in <a href="https://www.ftc.gov/news-events/events/2024/03/privacycon-2024">PrivacyCon 2024</a> and <a href="https://superagi.com/agi-leap-summit/">AGI Leap Summit</a>. (<a href="https://vimeo.com/922128271#t=11090s">Video Link</a>)
- **[Jan. 2024]** Two of our papers, "Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks" and "PRIME: Prioritizing Interpretability in Failure Mode Extraction" got accepted into ICLR 2024 Conference.
- **[Oct. 2023]** Our work "Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks"
  was covered in news articles such as <a href="https://www.wired.com/story/artificial-intelligence-watermarking-issues/">Wired</a>, 
  <a href="https://www.theregister.com/2023/10/02/watermarking_security_checks/">The Register</a>,
  and <a href="https://arstechnica.com/ai/2023/10/researchers-show-how-easy-it-is-to-defeat-ai-watermarks/">ArtTechnica</a>.
- **[Sep. 2023]** Our paper "ZeroGrad: Costless conscious remedies for catastrophic overfitting in the FGSM adversarial training" was published by Elsveir Journal.
- **[Jun. 2023]** Our paper "Robust Routing Made Easy: Reinforcing Networks Against Non-Benign Faults" was published by EEE/ACM Transactions on Networking.

{% include_relative _includes/publications.md %}
